{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B0q0WD6vRtFN"
   },
   "source": [
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "This software may be used and distributed according to the terms of the Llama 2 Community License Agreement."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tmVPz9atRtFS"
   },
   "source": [
    "## Quick Start Notebook\n",
    "\n",
    "This notebook shows how to train a Llama 2 model on a single GPU (e.g. A10 with 24GB) using int8 quantization and LoRA.\n",
    "\n",
    "### Step 0: Install pre-requirements and convert checkpoint\n",
    "\n",
    "The example uses the Hugging Face trainer and model which means that the checkpoint has to be converted from its original format into the dedicated Hugging Face format.\n",
    "The conversion can be achieved by running the `convert_llama_weights_to_hf.py` script provided with the transformer package.\n",
    "Given that the original checkpoint resides under `models/7B` we can install all requirements and convert the checkpoint with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C491JYjNHB9p",
    "outputId": "4ac9b956-1cea-41c8-f7a5-dafc754194b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinalDataset.csv  llama_recipes      __MACOSX\tmodels_hf.zip  tmp\n",
      "folder.zip\t  llama_recipes.zip  models_hf\tsample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SB9wexlURtFT",
    "outputId": "3378b95e-67df-4e48-ea88-bef8376f57ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (4.32.0.dev0)\n",
      "Requirement already satisfied: datasets in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (2.13.1)\n",
      "Requirement already satisfied: accelerate in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (0.1.98)\n",
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-py2.py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting py7zr\n",
      "  Obtaining dependency information for py7zr from https://files.pythonhosted.org/packages/2c/da/155bb1f692c067b9213c9c7b8c19a012a65027399606d623a25dfb1d3af1/py7zr-0.20.6-py3-none-any.whl.metadata\n",
      "  Downloading py7zr-0.20.6-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: scipy in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (1.11.1)\n",
      "Requirement already satisfied: peft in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (0.4.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (0.41.0)\n",
      "Requirement already satisfied: fire in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (0.5.0)\n",
      "Collecting torch_tb_profiler\n",
      "  Downloading torch_tb_profiler-0.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ipywidgets\n",
      "  Obtaining dependency information for ipywidgets from https://files.pythonhosted.org/packages/b8/d4/ce436660098b2f456e2b8fdf76d4f33cbc3766c874c4aa2f772c7a5e943f/ipywidgets-8.1.0-py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-8.1.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: filelock in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (1.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: psutil in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from accelerate) (2.0.1)\n",
      "Collecting texttable (from py7zr)\n",
      "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
      "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
      "  Obtaining dependency information for pycryptodomex>=3.6.6 from https://files.pythonhosted.org/packages/46/e6/7dbe57f21195e9f85c4d77ae995aa2a39dc45a2e34a978d98099c6ed43ce/pycryptodomex-3.18.0-cp35-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading pycryptodomex-3.18.0-cp35-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Collecting pyzstd>=0.14.4 (from py7zr)\n",
      "  Obtaining dependency information for pyzstd>=0.14.4 from https://files.pythonhosted.org/packages/13/46/7b4d3b1d2383026b6a53fbaf5e1e7c27b329a4a02b755389e4d131b085db/pyzstd-0.15.9-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pyzstd-0.15.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
      "  Downloading pyppmd-1.0.0-cp311-cp311-macosx_11_0_arm64.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n",
      "  Downloading pybcj-1.0.1-cp311-cp311-macosx_11_0_arm64.whl (23 kB)\n",
      "Collecting multivolumefile>=0.2.3 (from py7zr)\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting brotli>=1.0.9 (from py7zr)\n",
      "  Obtaining dependency information for brotli>=1.0.9 from https://files.pythonhosted.org/packages/96/12/ad41e7fadd5db55459c4c401842b47f7fee51068f86dd2894dd0dcfc2d2a/Brotli-1.1.0-cp311-cp311-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading Brotli-1.1.0-cp311-cp311-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Collecting inflate64>=0.3.1 (from py7zr)\n",
      "  Downloading inflate64-0.3.1-cp311-cp311-macosx_11_0_arm64.whl (35 kB)\n",
      "Requirement already satisfied: six in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from fire) (2.3.0)\n",
      "Collecting tensorboard!=2.1.0,>=1.15 (from torch_tb_profiler)\n",
      "  Obtaining dependency information for tensorboard!=2.1.0,>=1.15 from https://files.pythonhosted.org/packages/bc/a2/ff5f4c299eb37c95299a76015da3f30211468e29d8d6f1d011683279baee/tensorboard-2.14.0-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipywidgets) (8.14.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.7 (from ipywidgets)\n",
      "  Obtaining dependency information for widgetsnbextension~=4.0.7 from https://files.pythonhosted.org/packages/8e/d4/d31b12ac0b87e8cc9fdb6ea1eb6596de405eaaa2f25606aaa755d0eebbc0/widgetsnbextension-4.0.8-py3-none-any.whl.metadata\n",
      "  Downloading widgetsnbextension-4.0.8-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.7 (from ipywidgets)\n",
      "  Obtaining dependency information for jupyterlab-widgets~=3.0.7 from https://files.pythonhosted.org/packages/74/5e/2475ac62faf2e342b2bf20b8d8e375f49400ecb38f52e4e0a7557eb1cedb/jupyterlab_widgets-3.0.8-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_widgets-3.0.8-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from aiohttp->datasets) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: backcall in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
      "Requirement already satisfied: stack-data in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Collecting absl-py>=0.4 (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.48.2 (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/a1/9c/ef89aae6948949a891a50e19bb951aac2f7ceb9561fdfdcd07c9b890ed6c/grpcio-1.58.0-cp311-cp311-macosx_10_10_universal2.whl.metadata\n",
      "  Downloading grpcio-1.58.0-cp311-cp311-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (68.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/9b/59/a7c32e3d8d0e546a206e0552a2c04444544f15c1da4a01df8938d20c6ffc/werkzeug-2.3.7-py3-none-any.whl.metadata\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (0.38.4)\n",
      "Requirement already satisfied: sympy in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard!=2.1.0,>=1.15->torch_tb_profiler) (2.1.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard!=2.1.0,>=1.15->torch_tb_profiler)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading py7zr-0.20.6-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-macosx_10_9_universal2.whl (873 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m873.1/873.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.18.0-cp35-abi3-macosx_10_9_universal2.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyzstd-0.15.9-cp311-cp311-macosx_11_0_arm64.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.4/330.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.58.0-cp311-cp311-macosx_10_10_universal2.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: texttable, brotli, widgetsnbextension, werkzeug, urllib3, tensorboard-data-server, pyzstd, pyppmd, pycryptodomex, pybcj, pyasn1, protobuf, oauthlib, multivolumefile, markdown, jupyterlab-widgets, inflate64, grpcio, cachetools, absl-py, rsa, pyasn1-modules, py7zr, requests-oauthlib, google-auth, ipywidgets, google-auth-oauthlib, tensorboard, torch_tb_profiler\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.4\n",
      "    Uninstalling urllib3-2.0.4:\n",
      "      Successfully uninstalled urllib3-2.0.4\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "Successfully installed absl-py-1.4.0 brotli-1.1.0 cachetools-5.3.1 google-auth-2.22.0 google-auth-oauthlib-1.0.0 grpcio-1.58.0 inflate64-0.3.1 ipywidgets-8.1.0 jupyterlab-widgets-3.0.8 markdown-3.4.4 multivolumefile-0.2.3 oauthlib-3.2.2 protobuf-3.20.0 py7zr-0.20.6 pyasn1-0.5.0 pyasn1-modules-0.3.0 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.14.0 tensorboard-data-server-0.7.1 texttable-1.6.7 torch_tb_profiler-0.4.1 urllib3-1.26.16 werkzeug-2.3.7 widgetsnbextension-4.0.8\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install transformers datasets accelerate sentencepiece protobuf==3.20 py7zr scipy peft bitsandbytes fire torch_tb_profiler ipywidgets\n",
    "#TRANSFORM=`python -c \"import transformers;print('/'.join(transformers.__file__.split('/')[:-1])+'/models/llama/convert_llama_weights_to_hf.py')\"`\n",
    "#python ${TRANSFORM} --input_dir models --model_size 7B --output_dir models_hf/7B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yqbiLfqcRtFU"
   },
   "source": [
    "### Step 1: Load the model\n",
    "\n",
    "Point model_id to model weight folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi6Jl29mRPnm"
   },
   "outputs": [],
   "source": [
    "# !unzip models_hf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W8fK0mbEnmvG"
   },
   "outputs": [],
   "source": [
    "# Use a GCP bucket to load Llama model weights if needed\n",
    "# !curl https://sdk.cloud.google.com | bash\n",
    "# !gcloud init\n",
    "# !gsutil cp gs://adv-llama/models_hf.zip  .\n",
    "# !unzip models_hf.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QQpp5tnRtFU",
    "outputId": "da348f83-586c-4517-bbc5-a5ee6673c63c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advaithsridhar/anaconda3/envs/llm/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models_hf/7B'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m LlamaForCausalLM, LlamaTokenizer\n\u001b[1;32m      4\u001b[0m model_id\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./models_hf/7B\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m tokenizer \u001b[39m=\u001b[39m LlamaTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_id)\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1805\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[1;32m   1804\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1805\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   1806\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1807\u001b[0m             file_path,\n\u001b[1;32m   1808\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1809\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1810\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1811\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1812\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1813\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1814\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1815\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1816\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1817\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1818\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1819\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1820\u001b[0m         )\n\u001b[1;32m   1821\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[1;32m   1823\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/transformers/utils/hub.py:427\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    425\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    426\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    428\u001b[0m         path_or_repo_id,\n\u001b[1;32m    429\u001b[0m         filename,\n\u001b[1;32m    430\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    431\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    432\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    433\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    434\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    435\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    436\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    437\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    438\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    439\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRepo id must be a string, not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(repo_id)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models_hf/7B'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model_id=\"./models_hf/7B\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9e98a042e63e45dc9ff067deb18b5e69",
      "81e7a234c36142bbbbd29044a23dda88",
      "8ed6e209a92d4b42bccfe539f02c8d41",
      "8291749c7185430e801406195d26a82f",
      "79fd01396fba4e6da6c7fce76ce0d593",
      "c75b7a2ede39497989a7dc7b48571ce7",
      "f05f8a5be71a4e9d923501e2bb972848",
      "c7a091ed65e542eeb46215389461fd11",
      "b21d86dc49e04f9a86f9b614eb8ba5be",
      "03a78a5521e34d84a3c76cf570aa629d",
      "445e83ffaaf04773b23ffc10631e7aec"
     ]
    },
    "id": "gZnHW6tqfrHw",
    "outputId": "76328cdd-bd11-4828-d5dd-afea0609574f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e98a042e63e45dc9ff067deb18b5e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model =LlamaForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map='auto', torch_dtype=torch.float16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AAsPrHF8RtFV"
   },
   "source": [
    "### Step 2: Load the preprocessed dataset\n",
    "\n",
    "We load and preprocess the samsum dataset which consists of curated pairs of dialogs and their summarization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwSD-ukWqmvJ"
   },
   "outputs": [],
   "source": [
    "# !unzip folder.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeoqjCkNtZsr",
    "outputId": "406c3e3c-be51-4e77-9b26-2fee30edede2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: llama_recipes/ (stored 0%)\n",
      "updating: llama_recipes/multi_node.slurm (deflated 40%)\n",
      "updating: llama_recipes/configs/ (stored 0%)\n",
      "updating: llama_recipes/configs/peft.py (deflated 47%)\n",
      "updating: llama_recipes/configs/fsdp.py (deflated 46%)\n",
      "updating: llama_recipes/configs/__pycache__/ (stored 0%)\n",
      "updating: llama_recipes/configs/__pycache__/peft.cpython-310.pyc (deflated 46%)\n",
      "updating: llama_recipes/configs/__pycache__/training.cpython-310.pyc (deflated 41%)\n",
      "updating: llama_recipes/configs/__pycache__/datasets.cpython-310.pyc (deflated 54%)\n",
      "updating: llama_recipes/configs/__pycache__/fsdp.cpython-310.pyc (deflated 35%)\n",
      "updating: llama_recipes/configs/__pycache__/__init__.cpython-310.pyc (deflated 29%)\n",
      "updating: llama_recipes/configs/training.py (deflated 52%)\n",
      "updating: llama_recipes/configs/__init__.py (deflated 36%)\n",
      "updating: llama_recipes/configs/datasets.py (deflated 68%)\n",
      "updating: llama_recipes/docs/ (stored 0%)\n",
      "updating: llama_recipes/docs/FAQ.md (deflated 54%)\n",
      "updating: llama_recipes/docs/single_gpu.md (deflated 61%)\n",
      "updating: llama_recipes/docs/LLM_finetuning.md (deflated 57%)\n",
      "updating: llama_recipes/docs/Dataset.md (deflated 59%)\n",
      "updating: llama_recipes/docs/images/ (stored 0%)\n",
      "updating: llama_recipes/docs/images/featurebased_FN_.png (deflated 8%)\n",
      "updating: llama_recipes/docs/images/feature-based_FN.png (deflated 9%)\n",
      "updating: llama_recipes/docs/images/full-param-FN.png (deflated 9%)\n",
      "updating: llama_recipes/docs/multi_gpu.md (deflated 65%)\n",
      "updating: llama_recipes/docs/inference.md (deflated 63%)\n",
      "updating: llama_recipes/model_checkpointing/ (stored 0%)\n",
      "updating: llama_recipes/model_checkpointing/__pycache__/ (stored 0%)\n",
      "updating: llama_recipes/model_checkpointing/__pycache__/checkpoint_handler.cpython-310.pyc (deflated 50%)\n",
      "updating: llama_recipes/model_checkpointing/__pycache__/__init__.cpython-310.pyc (deflated 41%)\n",
      "updating: llama_recipes/model_checkpointing/__init__.py (deflated 47%)\n",
      "updating: llama_recipes/model_checkpointing/checkpoint_handler.py (deflated 71%)\n",
      "updating: llama_recipes/UPDATES.md (deflated 53%)\n",
      "updating: llama_recipes/inference/ (stored 0%)\n",
      "updating: llama_recipes/inference/vLLM_inference.py (deflated 58%)\n",
      "updating: llama_recipes/inference/samsum_prompt.txt (deflated 41%)\n",
      "updating: llama_recipes/inference/chats.json (deflated 58%)\n",
      "updating: llama_recipes/inference/chat_completion.py (deflated 66%)\n",
      "updating: llama_recipes/inference/inference.py (deflated 64%)\n",
      "updating: llama_recipes/inference/hf-text-generation-inference/ (stored 0%)\n",
      "updating: llama_recipes/inference/hf-text-generation-inference/merge_lora_weights.py (deflated 57%)\n",
      "updating: llama_recipes/inference/hf-text-generation-inference/README.md (deflated 57%)\n",
      "updating: llama_recipes/inference/safety_utils.py (deflated 67%)\n",
      "updating: llama_recipes/inference/chat_utils.py (deflated 60%)\n",
      "updating: llama_recipes/inference/checkpoint_converter_fsdp_hf.py (deflated 61%)\n",
      "updating: llama_recipes/inference/code-llama/ (stored 0%)\n",
      "updating: llama_recipes/inference/code-llama/code_completion_prompt.txt (deflated 25%)\n",
      "updating: llama_recipes/inference/code-llama/code_completion_example.py (deflated 64%)\n",
      "updating: llama_recipes/inference/code-llama/code_infilling_example.py (deflated 63%)\n",
      "updating: llama_recipes/inference/code-llama/code_infilling_prompt.txt (deflated 10%)\n",
      "updating: llama_recipes/inference/README.md (deflated 47%)\n",
      "updating: llama_recipes/inference/model_utils.py (deflated 55%)\n",
      "updating: llama_recipes/.gitignore (stored 0%)\n",
      "updating: llama_recipes/.github/ (stored 0%)\n",
      "updating: llama_recipes/.github/PULL_REQUEST_TEMPLATE.md (deflated 45%)\n",
      "updating: llama_recipes/.github/workflows/ (stored 0%)\n",
      "updating: llama_recipes/.github/workflows/spellcheck.yml (deflated 61%)\n",
      "updating: llama_recipes/.github/ISSUE_TEMPLATE/ (stored 0%)\n",
      "updating: llama_recipes/.github/ISSUE_TEMPLATE/feature-request.yml (deflated 54%)\n",
      "updating: llama_recipes/.github/ISSUE_TEMPLATE/bug.yml (deflated 61%)\n",
      "updating: llama_recipes/CONTRIBUTING.md (deflated 46%)\n",
      "updating: llama_recipes/LICENSE (deflated 57%)\n",
      "updating: llama_recipes/llama_finetuning.py (deflated 70%)\n",
      "updating: llama_recipes/.git/ (stored 0%)\n",
      "updating: llama_recipes/.git/info/ (stored 0%)\n",
      "updating: llama_recipes/.git/info/exclude (deflated 28%)\n",
      "updating: llama_recipes/.git/index (deflated 47%)\n",
      "updating: llama_recipes/.git/packed-refs (deflated 45%)\n",
      "updating: llama_recipes/.git/config (deflated 38%)\n",
      "updating: llama_recipes/.git/description (deflated 14%)\n",
      "updating: llama_recipes/.git/HEAD (stored 0%)\n",
      "updating: llama_recipes/.git/hooks/ (stored 0%)\n",
      "updating: llama_recipes/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
      "updating: llama_recipes/.git/hooks/pre-commit.sample (deflated 45%)\n",
      "updating: llama_recipes/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
      "updating: llama_recipes/.git/hooks/pre-receive.sample (deflated 40%)\n",
      "updating: llama_recipes/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
      "updating: llama_recipes/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
      "updating: llama_recipes/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
      "updating: llama_recipes/.git/hooks/sendemail-validate.sample (deflated 58%)\n",
      "updating: llama_recipes/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
      "updating: llama_recipes/.git/hooks/post-update.sample (deflated 27%)\n",
      "updating: llama_recipes/.git/hooks/pre-rebase.sample (deflated 59%)\n",
      "updating: llama_recipes/.git/hooks/pre-push.sample (deflated 49%)\n",
      "updating: llama_recipes/.git/hooks/update.sample (deflated 68%)\n",
      "updating: llama_recipes/.git/hooks/commit-msg.sample (deflated 44%)\n",
      "updating: llama_recipes/.git/objects/ (stored 0%)\n",
      "updating: llama_recipes/.git/objects/info/ (stored 0%)\n",
      "updating: llama_recipes/.git/objects/pack/ (stored 0%)\n",
      "updating: llama_recipes/.git/objects/pack/pack-2f71bed02e72a770ccecad8e92f5fcbbd811112e.idx (deflated 3%)\n",
      "updating: llama_recipes/.git/objects/pack/pack-2f71bed02e72a770ccecad8e92f5fcbbd811112e.rev (deflated 46%)\n",
      "updating: llama_recipes/.git/objects/pack/pack-2f71bed02e72a770ccecad8e92f5fcbbd811112e.pack (deflated 1%)\n",
      "updating: llama_recipes/.git/refs/ (stored 0%)\n",
      "updating: llama_recipes/.git/refs/tags/ (stored 0%)\n",
      "updating: llama_recipes/.git/refs/remotes/ (stored 0%)\n",
      "updating: llama_recipes/.git/refs/remotes/origin/ (stored 0%)\n",
      "updating: llama_recipes/.git/refs/remotes/origin/HEAD (stored 0%)\n",
      "updating: llama_recipes/.git/refs/heads/ (stored 0%)\n",
      "updating: llama_recipes/.git/refs/heads/main (stored 0%)\n",
      "updating: llama_recipes/.git/logs/ (stored 0%)\n",
      "updating: llama_recipes/.git/logs/HEAD (deflated 32%)\n",
      "updating: llama_recipes/.git/logs/refs/ (stored 0%)\n",
      "updating: llama_recipes/.git/logs/refs/remotes/ (stored 0%)\n",
      "updating: llama_recipes/.git/logs/refs/remotes/origin/ (stored 0%)\n",
      "updating: llama_recipes/.git/logs/refs/remotes/origin/HEAD (deflated 32%)\n",
      "updating: llama_recipes/.git/logs/refs/heads/ (stored 0%)\n",
      "updating: llama_recipes/.git/logs/refs/heads/main (deflated 32%)\n",
      "updating: llama_recipes/scripts/ (stored 0%)\n",
      "updating: llama_recipes/scripts/spellcheck.sh (deflated 39%)\n",
      "updating: llama_recipes/scripts/markdown_link_check_config.json (deflated 48%)\n",
      "updating: llama_recipes/scripts/spellcheck_conf/ (stored 0%)\n",
      "updating: llama_recipes/scripts/spellcheck_conf/wordlist.txt (deflated 48%)\n",
      "updating: llama_recipes/scripts/spellcheck_conf/spellcheck.yaml (deflated 45%)\n",
      "updating: llama_recipes/quickstart.ipynb (deflated 77%)\n",
      "updating: llama_recipes/utils/ (stored 0%)\n",
      "updating: llama_recipes/utils/config_utils.py (deflated 65%)\n",
      "updating: llama_recipes/utils/__pycache__/ (stored 0%)\n",
      "updating: llama_recipes/utils/__pycache__/train_utils.cpython-310.pyc (deflated 50%)\n",
      "updating: llama_recipes/utils/__pycache__/memory_utils.cpython-310.pyc (deflated 48%)\n",
      "updating: llama_recipes/utils/__pycache__/fsdp_utils.cpython-310.pyc (deflated 41%)\n",
      "updating: llama_recipes/utils/__pycache__/dataset_utils.cpython-310.pyc (deflated 38%)\n",
      "updating: llama_recipes/utils/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
      "updating: llama_recipes/utils/__init__.py (deflated 37%)\n",
      "updating: llama_recipes/utils/memory_utils.py (deflated 65%)\n",
      "updating: llama_recipes/utils/train_utils.py (deflated 73%)\n",
      "updating: llama_recipes/utils/dataset_utils.py (deflated 57%)\n",
      "updating: llama_recipes/utils/fsdp_utils.py (deflated 61%)\n",
      "updating: llama_recipes/README.md (deflated 62%)\n",
      "updating: llama_recipes/ft_datasets/ (stored 0%)\n",
      "updating: llama_recipes/ft_datasets/whatsapp_dataset.py (deflated 51%)\n",
      "updating: llama_recipes/ft_datasets/__pycache__/ (stored 0%)\n",
      "updating: llama_recipes/ft_datasets/__pycache__/whatsapp_dataset.cpython-310.pyc (deflated 40%)\n",
      "updating: llama_recipes/ft_datasets/__pycache__/samsum_dataset.cpython-310.pyc (deflated 41%)\n",
      "updating: llama_recipes/ft_datasets/__pycache__/utils.cpython-310.pyc (deflated 52%)\n",
      "updating: llama_recipes/ft_datasets/__pycache__/alpaca_dataset.cpython-310.pyc (deflated 43%)\n",
      "updating: llama_recipes/ft_datasets/__pycache__/__init__.cpython-310.pyc (deflated 40%)\n",
      "updating: llama_recipes/ft_datasets/__init__.py (deflated 51%)\n",
      "updating: llama_recipes/ft_datasets/samsum_dataset.py (deflated 53%)\n",
      "updating: llama_recipes/ft_datasets/alpaca_dataset.py (deflated 63%)\n",
      "updating: llama_recipes/ft_datasets/grammar_dataset/ (stored 0%)\n",
      "updating: llama_recipes/ft_datasets/grammar_dataset/__pycache__/ (stored 0%)\n",
      "updating: llama_recipes/ft_datasets/grammar_dataset/__pycache__/grammar_dataset.cpython-310.pyc (deflated 43%)\n",
      "updating: llama_recipes/ft_datasets/grammar_dataset/__pycache__/__init__.cpython-310.pyc (deflated 27%)\n",
      "updating: llama_recipes/ft_datasets/grammar_dataset/__init__.py (deflated 25%)\n",
      "updating: llama_recipes/ft_datasets/grammar_dataset/grammar_dataset_process.ipynb (deflated 77%)\n",
      "updating: llama_recipes/ft_datasets/grammar_dataset/grammar_dataset.py (deflated 60%)\n",
      "updating: llama_recipes/ft_datasets/utils.py (deflated 67%)\n",
      "updating: llama_recipes/ft_datasets/.ipynb_checkpoints/ (stored 0%)\n",
      "updating: llama_recipes/USE_POLICY.md (deflated 59%)\n",
      "updating: llama_recipes/requirements.txt (deflated 24%)\n",
      "updating: llama_recipes/policies/ (stored 0%)\n",
      "updating: llama_recipes/policies/activation_checkpointing_functions.py (deflated 55%)\n",
      "updating: llama_recipes/policies/__pycache__/ (stored 0%)\n",
      "updating: llama_recipes/policies/__pycache__/mixed_precision.cpython-310.pyc (deflated 35%)\n",
      "updating: llama_recipes/policies/__pycache__/activation_checkpointing_functions.cpython-310.pyc (deflated 44%)\n",
      "updating: llama_recipes/policies/__pycache__/wrapping.cpython-310.pyc (deflated 41%)\n",
      "updating: llama_recipes/policies/__pycache__/__init__.cpython-310.pyc (deflated 27%)\n",
      "updating: llama_recipes/policies/__pycache__/anyprecision_optimizer.cpython-310.pyc (deflated 49%)\n",
      "updating: llama_recipes/policies/__init__.py (deflated 36%)\n",
      "updating: llama_recipes/policies/wrapping.py (deflated 57%)\n",
      "updating: llama_recipes/policies/anyprecision_optimizer.py (deflated 70%)\n",
      "updating: llama_recipes/policies/mixed_precision.py (deflated 62%)\n",
      "updating: llama_recipes/CODE_OF_CONDUCT.md (deflated 56%)\n"
     ]
    }
   ],
   "source": [
    "#!zip llama_recipes.zip -r llama_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgcblbmqRtFV"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from utils.dataset_utils import get_preprocessed_dataset\n",
    "from configs.datasets import whatsapp_dataset\n",
    "\n",
    "train_dataset = get_preprocessed_dataset(tokenizer, whatsapp_dataset, 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L8WtGiGXnlx0"
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QpW3YtQPRtFV"
   },
   "source": [
    "### Step 3: Check base model\n",
    "\n",
    "Run the base model on an example input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sFqag9JgRtFW",
    "outputId": "a416b5f0-a759-4768-90b7-85ede1595637"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reply to the following messages as the user Advaith. Provide just one reply, do not continue the conversation\n",
      "User (Ritu): Hello.\n",
      "\n",
      "Advaith: Hey Ritu! What's up?\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Reply to the following messages as the user Advaith. Provide just one reply, do not continue the conversation\n",
    "User (Ritu): Hello.\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100,do_sample=True)[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BOzlV2loRtFW"
   },
   "source": [
    "We can see that the base model only repeats the conversation.\n",
    "\n",
    "### Step 4: Prepare model for PEFT\n",
    "\n",
    "Let's prepare the model for Parameter Efficient Fine Tuning (PEFT):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBL7O8hXRtFW",
    "outputId": "d24405b8-5ddf-4c63-f647-cfdb820a6fc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "def create_peft_config(model):\n",
    "    from peft import (\n",
    "        get_peft_model,\n",
    "        LoraConfig,\n",
    "        TaskType,\n",
    "        prepare_model_for_int8_training,\n",
    "    )\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules = [\"q_proj\", \"v_proj\"]\n",
    "    )\n",
    "\n",
    "    # prepare int-8 model for training\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model, peft_config\n",
    "\n",
    "# create peft config\n",
    "model, lora_config = create_peft_config(model)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "MKpotspvRtFX",
    "tags": []
   },
   "source": [
    "### Step 5: Define an optional profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1YMlWYJRtFX"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from contextlib import nullcontext\n",
    "enable_profiler = False\n",
    "output_dir = \"tmp/llama-output\"\n",
    "\n",
    "config = {\n",
    "    'lora_config': lora_config,\n",
    "    'learning_rate': 3e-5,\n",
    "    'num_train_epochs': 1,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'per_device_train_batch_size': 2,\n",
    "    'gradient_checkpointing': False,\n",
    "}\n",
    "\n",
    "# Set up profiler\n",
    "if enable_profiler:\n",
    "    wait, warmup, active, repeat = 1, 1, 2, 1\n",
    "    total_steps = (wait + warmup + active) * (1 + repeat)\n",
    "    schedule =  torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=repeat)\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f\"{output_dir}/logs/tensorboard\"),\n",
    "        record_shapes=True,\n",
    "        profile_memory=True,\n",
    "        with_stack=True)\n",
    "\n",
    "    class ProfilerCallback(TrainerCallback):\n",
    "        def __init__(self, profiler):\n",
    "            self.profiler = profiler\n",
    "\n",
    "        def on_step_end(self, *args, **kwargs):\n",
    "            self.profiler.step()\n",
    "\n",
    "    profiler_callback = ProfilerCallback(profiler)\n",
    "else:\n",
    "    profiler = nullcontext()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A55wIxLKRtFX"
   },
   "source": [
    "### Step 6: Fine tune the model\n",
    "\n",
    "Here, we fine tune the model for a single epoch which takes a bit more than an hour on a A100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dn5YjPWKRtFY",
    "outputId": "4792d40b-d376-48d1-b1a0-956b1c09912e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='508' max='508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [508/508 7:02:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.816400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.814300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.782900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.799100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.803300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.794400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.754900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.775900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.797800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.755200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.771500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.789800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.807800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.783000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.788600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.789300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.760900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.770400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.784100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.791300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.806100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.758400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.791600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import default_data_collator, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "\n",
    "# Define training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=False,  # Use BF16 if available\n",
    "    # logging strategies\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    max_steps=total_steps if enable_profiler else -1,\n",
    "    **{k:v for k,v in config.items() if k != 'lora_config'}\n",
    ")\n",
    "\n",
    "with profiler:\n",
    "    # Create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "        callbacks=[profiler_callback] if enable_profiler else [],\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t66mOpryRtFY"
   },
   "source": [
    "### Step 7:\n",
    "Save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Jccro8uYtdkp",
    "outputId": "4be8aae9-35bb-4688-efb1-3c9009a206bb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'tmp/llama-output'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es6Cu3lDRtFY"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bJfkVJ4hRtFY"
   },
   "source": [
    "### Step 8:\n",
    "Try the fine tuned model on the same example again to see the learning progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eVbJDLc_YVi"
   },
   "outputs": [],
   "source": [
    "🔊🔊🔊 Welcome to the Turing test! 📜\n",
    "\n",
    "Can you distinguish your friend Advaith 👨🏽 from an AI Model 🤖? Take this test to find out! 🧪\n",
    "\n",
    "You can ask me 3️⃣ questions!. Both Advaith and the AI bot will answer each question. 🤔\n",
    "\n",
    "🪧 Example question: \n",
    "You: What's your favourite dish?\n",
    "Candidate A: Curd rice\n",
    "Candidate B: Pasta\n",
    "\n",
    "After 3 such questions, you need to guess which candidate was Advaith 👨🏽, and which one was the AI bot🦙!\n",
    "\n",
    "⚠️ Other rules: \n",
    "1. No questions about events after training cut-off date (Sept 2022) ❌\n",
    "2. No factual questions about yourself or Advaith (ex. What is Advaith's last name?) ❌\n",
    "\n",
    "Ask your first question below. 👇🏽 Best of luck!\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNOuTDrziwT0",
    "outputId": "5eed5a72-d7e9-4237-f27e-d901f571b957"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: .config/ (stored 0%)\n",
      "  adding: .config/configurations/ (stored 0%)\n",
      "  adding: .config/configurations/config_default (deflated 15%)\n",
      "  adding: .config/configurations/config_adv-llama (deflated 14%)\n",
      "  adding: .config/.last_survey_prompt.yaml (stored 0%)\n",
      "  adding: .config/default_configs.db (deflated 98%)\n",
      "  adding: .config/gce (stored 0%)\n",
      "  adding: .config/config_sentinel (stored 0%)\n",
      "  adding: .config/active_config (stored 0%)\n",
      "  adding: .config/.last_opt_in_prompt.yaml (stored 0%)\n",
      "  adding: .config/logs/ (stored 0%)\n",
      "  adding: .config/logs/2023.08.03/ (stored 0%)\n",
      "  adding: .config/logs/2023.08.03/13.44.48.234496.log (deflated 56%)\n",
      "  adding: .config/logs/2023.08.03/13.43.26.810905.log (deflated 91%)\n",
      "  adding: .config/logs/2023.08.03/13.43.52.311313.log (deflated 58%)\n",
      "  adding: .config/logs/2023.08.03/13.44.47.473323.log (deflated 57%)\n",
      "  adding: .config/logs/2023.08.03/13.44.15.972512.log (deflated 86%)\n",
      "  adding: .config/logs/2023.08.03/13.44.23.058287.log (deflated 57%)\n",
      "  adding: .config/logs/2023.08.30/ (stored 0%)\n",
      "  adding: .config/logs/2023.08.30/22.47.15.241755.log (deflated 72%)\n",
      "  adding: .config/logs/2023.08.30/22.44.28.500024.log (deflated 91%)\n",
      "  adding: .config/logs/2023.08.30/22.44.46.581461.log (deflated 58%)\n",
      "  adding: .config/.last_update_check.json (deflated 23%)\n",
      "  adding: .config/adv-llama_configs.db (deflated 98%)\n",
      "  adding: .config/access_tokens.db (deflated 90%)\n",
      "  adding: .config/legacy_credentials/ (stored 0%)\n",
      "  adding: .config/legacy_credentials/ashketchumadvaith@gmail.com/ (stored 0%)\n",
      "  adding: .config/legacy_credentials/ashketchumadvaith@gmail.com/.boto (deflated 11%)\n",
      "  adding: .config/legacy_credentials/ashketchumadvaith@gmail.com/adc.json (deflated 15%)\n",
      "  adding: .config/credentials.db (deflated 95%)\n",
      "  adding: FinalDataset.csv (deflated 84%)\n",
      "  adding: tmp/ (stored 0%)\n",
      "  adding: tmp/llama-output/ (stored 0%)\n",
      "  adding: tmp/llama-output/adapter_config.json (deflated 44%)\n",
      "  adding: tmp/llama-output/README.md (deflated 82%)\n",
      "  adding: tmp/llama-output/adapter_model.bin (deflated 7%)\n",
      "  adding: tmp/llama-output/logs/ (stored 0%)\n",
      "  adding: tmp/llama-output/logs/events.out.tfevents.1693486434.c0f800ff210c.1622.0 (deflated 61%)\n",
      "  adding: tmp/llama-output/logs/events.out.tfevents.1693576738.c0f800ff210c.2758.0 (deflated 62%)\n",
      "  adding: tmp/llama-output/logs/events.out.tfevents.1693437107.c0f800ff210c.6437.0 (deflated 60%)\n",
      "  adding: tmp/llama-output/logs/events.out.tfevents.1693437261.c0f800ff210c.7430.0 (deflated 61%)\n",
      "  adding: tmp/llama-output/logs/events.out.tfevents.1693591364.c0f800ff210c.2758.1 (deflated 63%)\n",
      "  adding: .ipynb_checkpoints/ (stored 0%)\n",
      "  adding: llama_recipes/ (stored 0%)\n",
      "  adding: llama_recipes/multi_node.slurm (deflated 40%)\n",
      "  adding: llama_recipes/configs/ (stored 0%)\n",
      "  adding: llama_recipes/configs/peft.py (deflated 47%)\n",
      "  adding: llama_recipes/configs/fsdp.py (deflated 46%)\n",
      "  adding: llama_recipes/configs/__pycache__/ (stored 0%)\n",
      "  adding: llama_recipes/configs/__pycache__/peft.cpython-310.pyc (deflated 46%)\n",
      "  adding: llama_recipes/configs/__pycache__/training.cpython-310.pyc (deflated 41%)\n",
      "  adding: llama_recipes/configs/__pycache__/datasets.cpython-310.pyc (deflated 54%)\n",
      "  adding: llama_recipes/configs/__pycache__/fsdp.cpython-310.pyc (deflated 35%)\n",
      "  adding: llama_recipes/configs/__pycache__/__init__.cpython-310.pyc (deflated 29%)\n",
      "  adding: llama_recipes/configs/training.py (deflated 52%)\n",
      "  adding: llama_recipes/configs/__init__.py (deflated 36%)\n",
      "  adding: llama_recipes/configs/datasets.py (deflated 68%)\n",
      "  adding: llama_recipes/docs/ (stored 0%)\n",
      "  adding: llama_recipes/docs/FAQ.md (deflated 54%)\n",
      "  adding: llama_recipes/docs/single_gpu.md (deflated 61%)\n",
      "  adding: llama_recipes/docs/LLM_finetuning.md (deflated 57%)\n",
      "  adding: llama_recipes/docs/Dataset.md (deflated 59%)\n",
      "  adding: llama_recipes/docs/images/ (stored 0%)\n",
      "  adding: llama_recipes/docs/images/featurebased_FN_.png (deflated 8%)\n",
      "  adding: llama_recipes/docs/images/feature-based_FN.png (deflated 9%)\n",
      "  adding: llama_recipes/docs/images/full-param-FN.png (deflated 9%)\n",
      "  adding: llama_recipes/docs/multi_gpu.md (deflated 65%)\n",
      "  adding: llama_recipes/docs/inference.md (deflated 63%)\n",
      "  adding: llama_recipes/model_checkpointing/ (stored 0%)\n",
      "  adding: llama_recipes/model_checkpointing/__pycache__/ (stored 0%)\n",
      "  adding: llama_recipes/model_checkpointing/__pycache__/checkpoint_handler.cpython-310.pyc (deflated 50%)\n",
      "  adding: llama_recipes/model_checkpointing/__pycache__/__init__.cpython-310.pyc (deflated 41%)\n",
      "  adding: llama_recipes/model_checkpointing/__init__.py (deflated 47%)\n",
      "  adding: llama_recipes/model_checkpointing/checkpoint_handler.py (deflated 71%)\n",
      "  adding: llama_recipes/UPDATES.md (deflated 53%)\n",
      "  adding: llama_recipes/inference/ (stored 0%)\n",
      "  adding: llama_recipes/inference/vLLM_inference.py (deflated 58%)\n",
      "  adding: llama_recipes/inference/samsum_prompt.txt (deflated 41%)\n",
      "  adding: llama_recipes/inference/chats.json (deflated 58%)\n",
      "  adding: llama_recipes/inference/chat_completion.py (deflated 66%)\n",
      "  adding: llama_recipes/inference/inference.py (deflated 64%)\n",
      "  adding: llama_recipes/inference/hf-text-generation-inference/ (stored 0%)\n",
      "  adding: llama_recipes/inference/hf-text-generation-inference/merge_lora_weights.py (deflated 57%)\n",
      "  adding: llama_recipes/inference/hf-text-generation-inference/README.md (deflated 57%)\n",
      "  adding: llama_recipes/inference/safety_utils.py (deflated 67%)\n",
      "  adding: llama_recipes/inference/chat_utils.py (deflated 60%)\n",
      "  adding: llama_recipes/inference/checkpoint_converter_fsdp_hf.py (deflated 61%)\n",
      "  adding: llama_recipes/inference/code-llama/ (stored 0%)\n",
      "  adding: llama_recipes/inference/code-llama/code_completion_prompt.txt (deflated 25%)\n",
      "  adding: llama_recipes/inference/code-llama/code_completion_example.py (deflated 64%)\n",
      "  adding: llama_recipes/inference/code-llama/code_infilling_example.py (deflated 63%)\n",
      "  adding: llama_recipes/inference/code-llama/code_infilling_prompt.txt (deflated 10%)\n",
      "  adding: llama_recipes/inference/README.md (deflated 47%)\n",
      "  adding: llama_recipes/inference/model_utils.py (deflated 55%)\n",
      "  adding: llama_recipes/.gitignore (stored 0%)\n",
      "  adding: llama_recipes/.github/ (stored 0%)\n",
      "  adding: llama_recipes/.github/PULL_REQUEST_TEMPLATE.md (deflated 45%)\n",
      "  adding: llama_recipes/.github/workflows/ (stored 0%)\n",
      "  adding: llama_recipes/.github/workflows/spellcheck.yml (deflated 61%)\n",
      "  adding: llama_recipes/.github/ISSUE_TEMPLATE/ (stored 0%)\n",
      "  adding: llama_recipes/.github/ISSUE_TEMPLATE/feature-request.yml (deflated 54%)\n",
      "  adding: llama_recipes/.github/ISSUE_TEMPLATE/bug.yml (deflated 61%)\n",
      "  adding: llama_recipes/CONTRIBUTING.md (deflated 46%)\n",
      "  adding: llama_recipes/LICENSE (deflated 57%)\n",
      "  adding: llama_recipes/llama_finetuning.py (deflated 70%)\n",
      "  adding: llama_recipes/.git/ (stored 0%)\n",
      "  adding: llama_recipes/.git/info/ (stored 0%)\n",
      "  adding: llama_recipes/.git/info/exclude (deflated 28%)\n",
      "  adding: llama_recipes/.git/index (deflated 47%)\n",
      "  adding: llama_recipes/.git/packed-refs (deflated 45%)\n",
      "  adding: llama_recipes/.git/config (deflated 38%)\n",
      "  adding: llama_recipes/.git/description (deflated 14%)\n",
      "  adding: llama_recipes/.git/HEAD (stored 0%)\n",
      "  adding: llama_recipes/.git/hooks/ (stored 0%)\n",
      "  adding: llama_recipes/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
      "  adding: llama_recipes/.git/hooks/pre-commit.sample (deflated 45%)\n",
      "  adding: llama_recipes/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
      "  adding: llama_recipes/.git/hooks/pre-receive.sample (deflated 40%)\n",
      "  adding: llama_recipes/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
      "  adding: llama_recipes/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
      "  adding: llama_recipes/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
      "  adding: llama_recipes/.git/hooks/sendemail-validate.sample (deflated 58%)\n",
      "  adding: llama_recipes/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
      "  adding: llama_recipes/.git/hooks/post-update.sample (deflated 27%)\n",
      "  adding: llama_recipes/.git/hooks/pre-rebase.sample (deflated 59%)\n",
      "  adding: llama_recipes/.git/hooks/pre-push.sample (deflated 49%)\n",
      "  adding: llama_recipes/.git/hooks/update.sample (deflated 68%)\n",
      "  adding: llama_recipes/.git/hooks/commit-msg.sample (deflated 44%)\n",
      "  adding: llama_recipes/.git/objects/ (stored 0%)\n",
      "  adding: llama_recipes/.git/objects/info/ (stored 0%)\n",
      "  adding: llama_recipes/.git/objects/pack/ (stored 0%)\n",
      "  adding: llama_recipes/.git/objects/pack/pack-2f71bed02e72a770ccecad8e92f5fcbbd811112e.idx (deflated 3%)\n",
      "  adding: llama_recipes/.git/objects/pack/pack-2f71bed02e72a770ccecad8e92f5fcbbd811112e.rev (deflated 46%)\n",
      "  adding: llama_recipes/.git/objects/pack/pack-2f71bed02e72a770ccecad8e92f5fcbbd811112e.pack (deflated 1%)\n",
      "  adding: llama_recipes/.git/refs/ (stored 0%)\n",
      "  adding: llama_recipes/.git/refs/tags/ (stored 0%)\n",
      "  adding: llama_recipes/.git/refs/remotes/ (stored 0%)\n",
      "  adding: llama_recipes/.git/refs/remotes/origin/ (stored 0%)\n",
      "  adding: llama_recipes/.git/refs/remotes/origin/HEAD (stored 0%)\n",
      "  adding: llama_recipes/.git/refs/heads/ (stored 0%)\n",
      "  adding: llama_recipes/.git/refs/heads/main (stored 0%)\n",
      "  adding: llama_recipes/.git/logs/ (stored 0%)\n",
      "  adding: llama_recipes/.git/logs/HEAD (deflated 32%)\n",
      "  adding: llama_recipes/.git/logs/refs/ (stored 0%)\n",
      "  adding: llama_recipes/.git/logs/refs/remotes/ (stored 0%)\n",
      "  adding: llama_recipes/.git/logs/refs/remotes/origin/ (stored 0%)\n",
      "  adding: llama_recipes/.git/logs/refs/remotes/origin/HEAD (deflated 32%)\n",
      "  adding: llama_recipes/.git/logs/refs/heads/ (stored 0%)\n",
      "  adding: llama_recipes/.git/logs/refs/heads/main (deflated 32%)\n",
      "  adding: llama_recipes/scripts/ (stored 0%)\n",
      "  adding: llama_recipes/scripts/spellcheck.sh (deflated 39%)\n",
      "  adding: llama_recipes/scripts/markdown_link_check_config.json (deflated 48%)\n",
      "  adding: llama_recipes/scripts/spellcheck_conf/ (stored 0%)\n",
      "  adding: llama_recipes/scripts/spellcheck_conf/wordlist.txt (deflated 48%)\n",
      "  adding: llama_recipes/scripts/spellcheck_conf/spellcheck.yaml (deflated 45%)\n",
      "  adding: llama_recipes/quickstart.ipynb (deflated 77%)\n",
      "  adding: llama_recipes/utils/ (stored 0%)\n",
      "  adding: llama_recipes/utils/config_utils.py (deflated 65%)\n",
      "  adding: llama_recipes/utils/__pycache__/ (stored 0%)\n",
      "  adding: llama_recipes/utils/__pycache__/train_utils.cpython-310.pyc (deflated 50%)\n",
      "  adding: llama_recipes/utils/__pycache__/memory_utils.cpython-310.pyc (deflated 48%)\n",
      "  adding: llama_recipes/utils/__pycache__/fsdp_utils.cpython-310.pyc (deflated 41%)\n",
      "  adding: llama_recipes/utils/__pycache__/dataset_utils.cpython-310.pyc (deflated 38%)\n",
      "  adding: llama_recipes/utils/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
      "  adding: llama_recipes/utils/__init__.py (deflated 37%)\n",
      "  adding: llama_recipes/utils/memory_utils.py (deflated 65%)\n",
      "  adding: llama_recipes/utils/train_utils.py (deflated 73%)\n",
      "  adding: llama_recipes/utils/dataset_utils.py (deflated 57%)\n",
      "  adding: llama_recipes/utils/fsdp_utils.py (deflated 61%)\n",
      "  adding: llama_recipes/README.md (deflated 62%)\n",
      "  adding: llama_recipes/ft_datasets/ (stored 0%)\n",
      "  adding: llama_recipes/ft_datasets/whatsapp_dataset.py (deflated 51%)\n",
      "  adding: llama_recipes/ft_datasets/__pycache__/ (stored 0%)\n",
      "  adding: llama_recipes/ft_datasets/__pycache__/whatsapp_dataset.cpython-310.pyc (deflated 39%)\n",
      "  adding: llama_recipes/ft_datasets/__pycache__/samsum_dataset.cpython-310.pyc (deflated 41%)\n",
      "  adding: llama_recipes/ft_datasets/__pycache__/utils.cpython-310.pyc (deflated 52%)\n",
      "  adding: llama_recipes/ft_datasets/__pycache__/alpaca_dataset.cpython-310.pyc (deflated 43%)\n",
      "  adding: llama_recipes/ft_datasets/__pycache__/__init__.cpython-310.pyc (deflated 40%)\n",
      "  adding: llama_recipes/ft_datasets/__init__.py (deflated 51%)\n",
      "  adding: llama_recipes/ft_datasets/samsum_dataset.py (deflated 53%)\n",
      "  adding: llama_recipes/ft_datasets/alpaca_dataset.py (deflated 63%)\n",
      "  adding: llama_recipes/ft_datasets/grammar_dataset/ (stored 0%)\n",
      "  adding: llama_recipes/ft_datasets/grammar_dataset/__pycache__/ (stored 0%)\n",
      "  adding: llama_recipes/ft_datasets/grammar_dataset/__pycache__/grammar_dataset.cpython-310.pyc (deflated 43%)\n",
      "  adding: llama_recipes/ft_datasets/grammar_dataset/__pycache__/__init__.cpython-310.pyc (deflated 27%)\n",
      "  adding: llama_recipes/ft_datasets/grammar_dataset/__init__.py (deflated 25%)\n",
      "  adding: llama_recipes/ft_datasets/grammar_dataset/grammar_dataset_process.ipynb (deflated 77%)\n",
      "  adding: llama_recipes/ft_datasets/grammar_dataset/grammar_dataset.py (deflated 60%)\n",
      "  adding: llama_recipes/ft_datasets/utils.py (deflated 67%)\n",
      "  adding: llama_recipes/ft_datasets/.ipynb_checkpoints/ (stored 0%)\n",
      "  adding: llama_recipes/USE_POLICY.md (deflated 59%)\n",
      "  adding: llama_recipes/requirements.txt (deflated 24%)\n",
      "  adding: llama_recipes/policies/ (stored 0%)\n",
      "  adding: llama_recipes/policies/activation_checkpointing_functions.py (deflated 55%)\n",
      "  adding: llama_recipes/policies/__pycache__/ (stored 0%)\n",
      "  adding: llama_recipes/policies/__pycache__/mixed_precision.cpython-310.pyc (deflated 35%)\n",
      "  adding: llama_recipes/policies/__pycache__/activation_checkpointing_functions.cpython-310.pyc (deflated 44%)\n",
      "  adding: llama_recipes/policies/__pycache__/wrapping.cpython-310.pyc (deflated 41%)\n",
      "  adding: llama_recipes/policies/__pycache__/__init__.cpython-310.pyc (deflated 27%)\n",
      "  adding: llama_recipes/policies/__pycache__/anyprecision_optimizer.cpython-310.pyc (deflated 49%)\n",
      "  adding: llama_recipes/policies/__init__.py (deflated 36%)\n",
      "  adding: llama_recipes/policies/wrapping.py (deflated 57%)\n",
      "  adding: llama_recipes/policies/anyprecision_optimizer.py (deflated 70%)\n",
      "  adding: llama_recipes/policies/mixed_precision.py (deflated 62%)\n",
      "  adding: llama_recipes/CODE_OF_CONDUCT.md (deflated 56%)\n",
      "  adding: __MACOSX/ (stored 0%)\n",
      "  adding: __MACOSX/models_hf/ (stored 0%)\n",
      "  adding: __MACOSX/models_hf/._.DS_Store (deflated 56%)\n",
      "  adding: sample_data/ (stored 0%)\n",
      "  adding: sample_data/anscombe.json (deflated 83%)\n",
      "  adding: sample_data/README.md (deflated 42%)\n",
      "  adding: sample_data/mnist_test.csv (deflated 88%)\n",
      "  adding: sample_data/california_housing_test.csv (deflated 76%)\n",
      "  adding: sample_data/california_housing_train.csv (deflated 79%)\n",
      "  adding: sample_data/mnist_train_small.csv (deflated 88%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r all_docs.zip . -x \"models_hf/*\" -x \"*.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc1YUdM2RtFZ",
    "outputId": "f91ebf88-d396-4508-ecce-827b4222770f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_prompt \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mReply to the following messages as the user Advaith. Provide just one reply, do not continue the conversation\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mUser (John): Who are you? :P\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mAdvaith:\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model_input \u001b[39m=\u001b[39m tokenizer(eval_prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     10\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "Reply to the following messages as the user Advaith. Provide just one reply, do not continue the conversation\n",
    "User (John): Who are you? :P\n",
    "Advaith:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=250,do_sample=True)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZd7otwhCs76"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03a78a5521e34d84a3c76cf570aa629d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "445e83ffaaf04773b23ffc10631e7aec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79fd01396fba4e6da6c7fce76ce0d593": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81e7a234c36142bbbbd29044a23dda88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c75b7a2ede39497989a7dc7b48571ce7",
      "placeholder": "​",
      "style": "IPY_MODEL_f05f8a5be71a4e9d923501e2bb972848",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8291749c7185430e801406195d26a82f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03a78a5521e34d84a3c76cf570aa629d",
      "placeholder": "​",
      "style": "IPY_MODEL_445e83ffaaf04773b23ffc10631e7aec",
      "value": " 2/2 [01:02&lt;00:00, 28.34s/it]"
     }
    },
    "8ed6e209a92d4b42bccfe539f02c8d41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7a091ed65e542eeb46215389461fd11",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b21d86dc49e04f9a86f9b614eb8ba5be",
      "value": 2
     }
    },
    "9e98a042e63e45dc9ff067deb18b5e69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81e7a234c36142bbbbd29044a23dda88",
       "IPY_MODEL_8ed6e209a92d4b42bccfe539f02c8d41",
       "IPY_MODEL_8291749c7185430e801406195d26a82f"
      ],
      "layout": "IPY_MODEL_79fd01396fba4e6da6c7fce76ce0d593"
     }
    },
    "b21d86dc49e04f9a86f9b614eb8ba5be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c75b7a2ede39497989a7dc7b48571ce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7a091ed65e542eeb46215389461fd11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f05f8a5be71a4e9d923501e2bb972848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
